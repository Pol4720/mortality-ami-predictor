% ============================================================================
% APÉNDICE B: HIPERPARÁMETROS Y CONFIGURACIÓN DE MODELOS
% ============================================================================

\section{Hiperparámetros y Configuración de Modelos}
\label{app:hiperparametros}

Este apéndice documenta los espacios de búsqueda de hiperparámetros y las configuraciones finales de todos los modelos evaluados.

\subsection{Configuración General}

\begin{table}[H]
\centering
\caption{Configuración general del proceso de modelado}
\label{tab:config_general}
\begin{tabular}{@{}ll@{}}
\toprule
\textbf{Parámetro} & \textbf{Valor} \\
\midrule
Semilla aleatoria (random\_state) & \placeholder{42} \\
Validación cruzada & \placeholder{5-fold} estratificada \\
Métrica de optimización & \placeholder{roc\_auc} \\
Método de búsqueda & \placeholder{RandomizedSearchCV / Optuna} \\
N iteraciones búsqueda & \placeholder{100} \\
Early stopping (donde aplica) & \placeholder{10 rondas sin mejora} \\
n\_jobs & \placeholder{-1 (todos los cores)} \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Regresión Logística}

\begin{table}[H]
\centering
\caption{Hiperparámetros -- Regresión Logística}
\label{tab:hp_lr_apendice}
\begin{tabular}{@{}llll@{}}
\toprule
\textbf{Hiperparámetro} & \textbf{Espacio de búsqueda} & \textbf{Valor óptimo} & \textbf{Descripción} \\
\midrule
penalty & [l1, l2, elasticnet] & \placeholder{l2} & Tipo de regularización \\
C & [0.001, 0.01, 0.1, 1, 10, 100] & \placeholder{1.0} & Inverso de fuerza de regularización \\
solver & [lbfgs, saga, liblinear] & \placeholder{lbfgs} & Algoritmo de optimización \\
class\_weight & [None, balanced] & \placeholder{balanced} & Ponderación de clases \\
max\_iter & [500, 1000, 2000] & \placeholder{1000} & Máximo de iteraciones \\
l1\_ratio & [0.0, 0.25, 0.5, 0.75, 1.0] & \placeholder{N/A} & Ratio L1/L2 (si elasticnet) \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Random Forest}

\begin{table}[H]
\centering
\caption{Hiperparámetros -- Random Forest}
\label{tab:hp_rf_apendice}
\begin{tabular}{@{}llll@{}}
\toprule
\textbf{Hiperparámetro} & \textbf{Espacio de búsqueda} & \textbf{Valor óptimo} & \textbf{Descripción} \\
\midrule
n\_estimators & [100, 200, 300, 500, 1000] & \placeholder{500} & Número de árboles \\
max\_depth & [None, 5, 10, 15, 20, 25] & \placeholder{15} & Profundidad máxima \\
min\_samples\_split & [2, 5, 10, 20] & \placeholder{5} & Min muestras para split \\
min\_samples\_leaf & [1, 2, 4, 8] & \placeholder{2} & Min muestras por hoja \\
max\_features & [sqrt, log2, 0.3, 0.5, 0.7] & \placeholder{sqrt} & Features por split \\
class\_weight & [balanced, balanced\_subsample] & \placeholder{balanced} & Ponderación de clases \\
bootstrap & [True, False] & \placeholder{True} & Bootstrap sampling \\
criterion & [gini, entropy] & \placeholder{gini} & Criterio de split \\
\bottomrule
\end{tabular}
\end{table}

\subsection{XGBoost}

\begin{table}[H]
\centering
\caption{Hiperparámetros -- XGBoost}
\label{tab:hp_xgb_apendice}
\begin{tabular}{@{}lp{4cm}ll@{}}
\toprule
\textbf{Hiperparámetro} & \textbf{Espacio de búsqueda} & \textbf{Valor óptimo} & \textbf{Descripción} \\
\midrule
n\_estimators & [100, 200, 300, 500, 1000] & \placeholder{300} & Número de rondas boosting \\
max\_depth & [3, 4, 5, 6, 7, 8, 10] & \placeholder{5} & Profundidad máxima \\
learning\_rate (eta) & [0.01, 0.02, 0.05, 0.1, 0.2] & \placeholder{0.05} & Tasa de aprendizaje \\
subsample & [0.6, 0.7, 0.8, 0.9, 1.0] & \placeholder{0.8} & Fracción de muestras \\
colsample\_bytree & [0.6, 0.7, 0.8, 0.9, 1.0] & \placeholder{0.8} & Fracción de features \\
colsample\_bylevel & [0.6, 0.7, 0.8, 0.9, 1.0] & \placeholder{1.0} & Features por nivel \\
min\_child\_weight & [1, 3, 5, 7, 10] & \placeholder{5} & Min suma de pesos hijos \\
gamma & [0, 0.1, 0.2, 0.3, 0.5] & \placeholder{0.1} & Min reducción de pérdida \\
reg\_alpha & [0, 0.001, 0.01, 0.1, 1] & \placeholder{0.01} & Regularización L1 \\
reg\_lambda & [0, 0.001, 0.01, 0.1, 1] & \placeholder{0.1} & Regularización L2 \\
scale\_pos\_weight & [1, ratio\_clases] & \placeholder{ratio} & Balance de clases \\
objective & -- & binary:logistic & Función objetivo \\
eval\_metric & -- & auc, logloss & Métricas de evaluación \\
\bottomrule
\end{tabular}
\end{table}

\subsection{LightGBM}

\begin{table}[H]
\centering
\caption{Hiperparámetros -- LightGBM}
\label{tab:hp_lgbm_apendice}
\begin{tabular}{@{}lp{4cm}ll@{}}
\toprule
\textbf{Hiperparámetro} & \textbf{Espacio de búsqueda} & \textbf{Valor óptimo} & \textbf{Descripción} \\
\midrule
n\_estimators & [100, 200, 300, 500, 1000] & \placeholder{400} & Número de iteraciones \\
num\_leaves & [31, 50, 70, 100, 150] & \placeholder{70} & Hojas por árbol \\
max\_depth & [-1, 5, 10, 15, 20] & \placeholder{10} & Profundidad máxima \\
learning\_rate & [0.01, 0.02, 0.05, 0.1, 0.2] & \placeholder{0.05} & Tasa de aprendizaje \\
min\_child\_samples & [5, 10, 20, 50, 100] & \placeholder{20} & Min datos por hoja \\
min\_child\_weight & [0.001, 0.01, 0.1] & \placeholder{0.01} & Min suma hessian \\
subsample & [0.6, 0.7, 0.8, 0.9, 1.0] & \placeholder{0.8} & Fracción muestras \\
colsample\_bytree & [0.6, 0.7, 0.8, 0.9, 1.0] & \placeholder{0.8} & Fracción features \\
reg\_alpha & [0, 0.01, 0.1, 1] & \placeholder{0.1} & Regularización L1 \\
reg\_lambda & [0, 0.01, 0.1, 1] & \placeholder{0.1} & Regularización L2 \\
class\_weight & [None, balanced] & \placeholder{balanced} & Ponderación clases \\
boosting\_type & [gbdt, dart] & \placeholder{gbdt} & Tipo de boosting \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Red Neuronal (MLP)}

\begin{table}[H]
\centering
\caption{Hiperparámetros -- Red Neuronal}
\label{tab:hp_nn_apendice}
\begin{tabular}{@{}lp{4cm}ll@{}}
\toprule
\textbf{Hiperparámetro} & \textbf{Espacio de búsqueda} & \textbf{Valor óptimo} & \textbf{Descripción} \\
\midrule
hidden\_layer\_sizes & [(64,32), (128,64), (128,64,32), (256,128,64)] & \placeholder{(128,64,32)} & Arquitectura \\
activation & [relu, tanh, selu] & \placeholder{relu} & Función activación \\
solver & [adam, sgd] & \placeholder{adam} & Optimizador \\
alpha & [0.0001, 0.001, 0.01, 0.1] & \placeholder{0.001} & Regularización L2 \\
learning\_rate & [constant, adaptive] & \placeholder{adaptive} & Esquema LR \\
learning\_rate\_init & [0.001, 0.01, 0.1] & \placeholder{0.001} & LR inicial \\
batch\_size & [32, 64, 128, 256] & \placeholder{64} & Tamaño batch \\
max\_iter & [200, 500, 1000] & \placeholder{500} & Máx épocas \\
early\_stopping & [True] & True & Early stopping \\
n\_iter\_no\_change & [10, 20, 30] & \placeholder{20} & Paciencia ES \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Configuración de AutoML (si aplica)}

\begin{placeholderblock}
\textbf{[COMPLETAR SI SE USÓ AUTOML]}

\begin{table}[H]
\centering
\caption{Configuración de AutoML}
\label{tab:config_automl}
\begin{tabular}{@{}ll@{}}
\toprule
\textbf{Parámetro} & \textbf{Valor} \\
\midrule
Framework & \placeholder{AutoKeras / Auto-sklearn / H2O} \\
Tiempo máximo de búsqueda & \placeholder{X horas} \\
Máximo de trials/modelos & \placeholder{XX} \\
Métrica de optimización & \placeholder{roc\_auc} \\
Mejor arquitectura encontrada & \placeholder{Descripción} \\
\bottomrule
\end{tabular}
\end{table}
\end{placeholderblock}

\subsection{Configuración de Calibración}

\begin{table}[H]
\centering
\caption{Configuración de calibración de probabilidades}
\label{tab:config_calibracion}
\begin{tabular}{@{}ll@{}}
\toprule
\textbf{Parámetro} & \textbf{Valor} \\
\midrule
Método & \placeholder{CalibratedClassifierCV} \\
cv & \placeholder{5} \\
method & \placeholder{sigmoid / isotonic} \\
ensemble & \placeholder{True/False} \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Configuración de SHAP}

\begin{table}[H]
\centering
\caption{Configuración de análisis SHAP}
\label{tab:config_shap}
\begin{tabular}{@{}ll@{}}
\toprule
\textbf{Parámetro} & \textbf{Valor} \\
\midrule
Explainer type & \placeholder{TreeExplainer / KernelExplainer} \\
Data para background & \placeholder{shap.sample(X\_train, 100)} \\
N muestras para summary plot & \placeholder{N} \\
Cálculo de interacciones & \placeholder{Sí/No} \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Reproducibilidad}

Para garantizar la reproducibilidad de los experimentos:

\begin{lstlisting}[language=Python, caption=Configuración de semillas aleatorias]
import numpy as np
import random
import os

# Seed configuration
RANDOM_SEED = 42

def set_seed(seed=RANDOM_SEED):
    np.random.seed(seed)
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    # For TensorFlow/Keras (if used):
    # tf.random.set_seed(seed)
    # For PyTorch (if used):
    # torch.manual_seed(seed)

set_seed()
\end{lstlisting}
