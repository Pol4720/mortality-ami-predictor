{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce19a4d2",
   "metadata": {},
   "source": [
    "# AutoML Mortality Prediction (Colab/Kaggle Ready)\n",
    "\n",
    "Este notebook implementa un flujo de trabajo de AutoML para predecir la mortalidad intrahospitalaria (`mortality_inhospital`).\n",
    "\n",
    "## Técnicas de Resampling:\n",
    "- SMOTEENN\n",
    "- Borderline-SMOTE + TomekLinks\n",
    "- Borderline-SMOTE\n",
    "\n",
    "## Modelos:\n",
    "- Logistic Regression (LR)\n",
    "- Random Forest (RF)\n",
    "- XGBoost (XGB)\n",
    "- Support Vector Machine (SVM)\n",
    "- Multi-Layer Perceptron (MLP)\n",
    "- Gradient Boosting Machine (GBM)\n",
    "- Bootstrap Aggregating (Bagging)\n",
    "\n",
    "## Métricas:\n",
    "- Accuracy\n",
    "- Recall\n",
    "- AUC-ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146138df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalación de librerías necesarias\n",
    "# En Kaggle/Colab: Ejecuta esta celda para actualizar las versiones compatibles\n",
    "!pip install -q --upgrade scikit-learn imbalanced-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60819d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalación de librerías necesarias\n",
    "# En Kaggle: Las librerías ya vienen preinstaladas, puedes saltar esta celda\n",
    "# En Colab: Descomenta la siguiente línea si es necesario\n",
    "# !pip install -q imbalanced-learn xgboost pandas scikit-learn matplotlib seaborn joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f0a9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, accuracy_score, recall_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Modelos\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, BaggingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import xgboost as xgb\n",
    "\n",
    "# Resampling\n",
    "from imblearn.combine import SMOTEENN\n",
    "from imblearn.over_sampling import BorderlineSMOTE\n",
    "from imblearn.under_sampling import TomekLinks\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import joblib\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e556265e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carga de datos\n",
    "# Si estás en Colab, sube el archivo 'cleaned_dataset_20251104_200505.csv'\n",
    "try:\n",
    "    df = pd.read_csv('/content/drive/MyDrive/data/cleaned_dataset_20251104_200505.csv')\n",
    "    print(\"Dataset cargado localmente.\")\n",
    "except FileNotFoundError:\n",
    "    try:\n",
    "        from google.colab import files\n",
    "        print(\"Sube el archivo 'cleaned_dataset_20251104_200505.csv':\")\n",
    "        uploaded = files.upload()\n",
    "        df = pd.read_csv(next(iter(uploaded.keys())))\n",
    "    except ImportError:\n",
    "        print(\"No se encontró el archivo y no estás en Google Colab. Por favor verifica la ruta.\")\n",
    "\n",
    "print(f\"Dimensiones del dataset: {df.shape}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9cc0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocesamiento\n",
    "target = 'mortality_inhospital'\n",
    "X = df.drop(columns=[target])\n",
    "y = df[target]\n",
    "\n",
    "# Imputación de valores faltantes (Mediana)\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "X_imputed = pd.DataFrame(imputer.fit_transform(X), columns=X.columns)\n",
    "\n",
    "# Escalado de datos\n",
    "scaler = StandardScaler()\n",
    "X_scaled = pd.DataFrame(scaler.fit_transform(X_imputed), columns=X.columns)\n",
    "\n",
    "print(\"Distribución de clases original:\")\n",
    "print(y.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e946a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definición de técnicas de Resampling\n",
    "# Nota: Para métodos compuestos usamos una lista de tuplas (pasos individuales)\n",
    "# 'None' usa un paso \"passthrough\" que no modifica los datos\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "resampling_methods = {\n",
    "    'None (Original)': [],  # Sin resampling - datos originales desbalanceados\n",
    "    'SMOTE': [('smote', SMOTE(random_state=42))],\n",
    "    'SMOTEENN': [('smoteenn', SMOTEENN(random_state=42))],\n",
    "    'BorderlineSMOTE': [('borderline', BorderlineSMOTE(random_state=42, kind='borderline-1'))],\n",
    "    'BorderlineSMOTE+Tomek': [\n",
    "        ('borderline', BorderlineSMOTE(random_state=42, kind='borderline-1')),\n",
    "        ('tomek', TomekLinks())\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Definición de Modelos\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
    "    'Random Forest': RandomForestClassifier(random_state=42, n_estimators=100),\n",
    "    'XGBoost': xgb.XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss'),\n",
    "    'SVM': SVC(random_state=42, probability=True),\n",
    "    'MLP': MLPClassifier(random_state=42, max_iter=1000),\n",
    "    'GBM': GradientBoostingClassifier(random_state=42),\n",
    "    'Bagging': BaggingClassifier(random_state=42)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19f9501",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración de directorios para guardar modelos y resultados\n",
    "MODELS_DIR = Path('trained_models')\n",
    "RESULTS_FILE = 'training_results.csv'\n",
    "\n",
    "# Crear directorio si no existe\n",
    "MODELS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "def get_model_filename(res_name, model_name):\n",
    "    \"\"\"Genera nombre de archivo seguro para el modelo\"\"\"\n",
    "    safe_res = res_name.replace(' ', '_').replace('(', '').replace(')', '').replace('+', '_')\n",
    "    safe_model = model_name.replace(' ', '_')\n",
    "    return MODELS_DIR / f\"{safe_res}_{safe_model}.joblib\"\n",
    "\n",
    "def load_or_train_model(res_name, res_steps, model_name, model, X, y, cv):\n",
    "    \"\"\"Carga modelo si existe, sino lo entrena y guarda\"\"\"\n",
    "    model_path = get_model_filename(res_name, model_name)\n",
    "    \n",
    "    # Verificar si ya existe el modelo entrenado\n",
    "    if model_path.exists():\n",
    "        print(f\"  - Cargando {model_name} desde cache...\")\n",
    "        saved_data = joblib.load(model_path)\n",
    "        return saved_data['pipeline'], saved_data['scores']\n",
    "    \n",
    "    print(f\"  - Entrenando {model_name}...\")\n",
    "    \n",
    "    # Crear pipeline\n",
    "    if len(res_steps) == 0:\n",
    "        from sklearn.pipeline import Pipeline as SkPipeline\n",
    "        pipeline = SkPipeline([('classifier', model)])\n",
    "    else:\n",
    "        pipeline_steps = res_steps + [('classifier', model)]\n",
    "        pipeline = ImbPipeline(pipeline_steps)\n",
    "    \n",
    "    # Calcular métricas con CV\n",
    "    auc_scores = cross_val_score(pipeline, X, y, cv=cv, scoring='roc_auc')\n",
    "    recall_scores = cross_val_score(pipeline, X, y, cv=cv, scoring='recall')\n",
    "    accuracy_scores = cross_val_score(pipeline, X, y, cv=cv, scoring='accuracy')\n",
    "    \n",
    "    # Entrenar modelo final con todos los datos\n",
    "    pipeline.fit(X, y)\n",
    "    \n",
    "    scores = {\n",
    "        'auc': auc_scores,\n",
    "        'recall': recall_scores,\n",
    "        'accuracy': accuracy_scores\n",
    "    }\n",
    "    \n",
    "    # Guardar modelo y scores\n",
    "    joblib.dump({'pipeline': pipeline, 'scores': scores}, model_path)\n",
    "    print(f\"    ✓ Modelo guardado en {model_path}\")\n",
    "    \n",
    "    return pipeline, scores\n",
    "\n",
    "# Loop de Entrenamiento y Evaluación\n",
    "results = []\n",
    "trained_pipelines = {}  # Diccionario para almacenar pipelines entrenados\n",
    "\n",
    "# Validación Cruzada Estratificada\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Verificar si ya existen resultados previos\n",
    "if os.path.exists(RESULTS_FILE):\n",
    "    print(f\"Encontrado archivo de resultados previos: {RESULTS_FILE}\")\n",
    "    results_df = pd.read_csv(RESULTS_FILE)\n",
    "    print(f\"Cargados {len(results_df)} resultados previos.\")\n",
    "    \n",
    "    # Cargar modelos existentes\n",
    "    for res_name in resampling_methods.keys():\n",
    "        for model_name in models.keys():\n",
    "            model_path = get_model_filename(res_name, model_name)\n",
    "            if model_path.exists():\n",
    "                saved_data = joblib.load(model_path)\n",
    "                trained_pipelines[f\"{res_name}_{model_name}\"] = saved_data['pipeline']\n",
    "else:\n",
    "    print(\"Iniciando entrenamiento desde cero...\")\n",
    "    \n",
    "    for res_name, res_steps in resampling_methods.items():\n",
    "        print(f\"\\nEvaluando técnica de resampling: {res_name}\")\n",
    "        \n",
    "        for model_name, model in models.items():\n",
    "            try:\n",
    "                pipeline, scores = load_or_train_model(\n",
    "                    res_name, res_steps, model_name, model, X_scaled, y, cv\n",
    "                )\n",
    "                \n",
    "                trained_pipelines[f\"{res_name}_{model_name}\"] = pipeline\n",
    "                \n",
    "                results.append({\n",
    "                    'Resampling': res_name,\n",
    "                    'Model': model_name,\n",
    "                    'AUC Mean': scores['auc'].mean(),\n",
    "                    'AUC Std': scores['auc'].std(),\n",
    "                    'Recall Mean': scores['recall'].mean(),\n",
    "                    'Recall Std': scores['recall'].std(),\n",
    "                    'Accuracy Mean': scores['accuracy'].mean(),\n",
    "                    'Accuracy Std': scores['accuracy'].std()\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(f\"Error entrenando {model_name} con {res_name}: {e}\")\n",
    "    \n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    # Guardar resultados en CSV\n",
    "    results_df.to_csv(RESULTS_FILE, index=False)\n",
    "    print(f\"\\n✓ Resultados guardados en {RESULTS_FILE}\")\n",
    "\n",
    "print(f\"\\nEntrenamiento completado. {len(trained_pipelines)} modelos disponibles.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c5b015",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualización de Resultados\n",
    "if not results_df.empty:\n",
    "    results_df_sorted = results_df.sort_values(by='AUC Mean', ascending=False)\n",
    "\n",
    "    print(\"Top 10 Modelos por AUC:\")\n",
    "    display(results_df_sorted.head(10))\n",
    "\n",
    "    # Gráfico de Barras para AUC\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    sns.barplot(data=results_df, x='Model', y='AUC Mean', hue='Resampling')\n",
    "    plt.title('Comparación de AUC por Modelo y Técnica de Resampling')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.ylim(0.5, 1.0)\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Gráfico de Barras para Recall\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    sns.barplot(data=results_df, x='Model', y='Recall Mean', hue='Resampling')\n",
    "    plt.title('Comparación de Recall por Modelo y Técnica de Resampling')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.ylim(0.0, 1.0)\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No se obtuvieron resultados.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d463327",
   "metadata": {},
   "source": [
    "## Utilidades: Forzar Reentrenamiento\n",
    "\n",
    "Si necesitas reentrenar todos los modelos desde cero, ejecuta la siguiente celda para eliminar el cache."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c524e8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ⚠️ EJECUTAR SOLO SI QUIERES REENTRENAR TODO DESDE CERO\n",
    "# # Esta celda elimina todos los modelos guardados y el archivo de resultados\n",
    "\n",
    "# FORCE_RETRAIN = False  # Cambiar a True para forzar reentrenamiento\n",
    "\n",
    "# if FORCE_RETRAIN:\n",
    "#     import shutil\n",
    "    \n",
    "#     # Eliminar directorio de modelos\n",
    "#     if MODELS_DIR.exists():\n",
    "#         shutil.rmtree(MODELS_DIR)\n",
    "#         print(f\"✓ Directorio {MODELS_DIR} eliminado\")\n",
    "    \n",
    "#     # Eliminar archivo de resultados\n",
    "#     if os.path.exists(RESULTS_FILE):\n",
    "#         os.remove(RESULTS_FILE)\n",
    "#         print(f\"✓ Archivo {RESULTS_FILE} eliminado\")\n",
    "    \n",
    "#     print(\"\\n⚠️ Cache eliminado. Vuelve a ejecutar la celda de entrenamiento.\")\n",
    "# else:\n",
    "#     print(\"FORCE_RETRAIN está en False. Cambia a True y ejecuta para eliminar el cache.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
